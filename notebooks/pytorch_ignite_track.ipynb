{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMov-LUSabxx"
   },
   "source": [
    "# Convolutional Neural Networks for Classifying Fashion-MNIST Dataset using Ignite\n",
    "# Experiment tracking and visualization using Aim\n",
    "This is a tutorial on using Ignite to train neural network models, setup experiments, validate models and then visualizing experiment data with Aim.\n",
    "\n",
    "In this notebook, we will be doing classification of images using Convolutional Neural Networks \n",
    "\n",
    "We will be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) Fashion-MNIST is a set of 28x28 grayscale images of clothes.\n",
    "\n",
    "![Fashion MNIST dataset](https://github.com/abdulelahsm/ignite/blob/update-tutorials/examples/notebooks/assets/fashion-mnist.png?raw=1)\n",
    "\n",
    "Lets get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXZxwFvCGqz0"
   },
   "source": [
    "## Required Dependencies\n",
    "\n",
    "We assume that `torch`, `ignite` and `aim` are already installed. We can install them using `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7LJdJfU6pra"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install aim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PiU02-fabxz"
   },
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp_4isvHabx5"
   },
   "source": [
    "We import `torch`, `nn` and `functional` modules to create our models.\n",
    "\n",
    "We also import `datasets` and `transforms` from torchvision for loading the dataset and applying transforms to the images in the dataset.\n",
    "\n",
    "We import `Dataloader` for making train and validation loader for loading data into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-OllAGT6pr-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import ignite\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pj7oLY36abx8"
   },
   "source": [
    "`Ignite` is a High-level library to help with training neural networks in PyTorch. It comes with an `Engine` to setup a training loop, various metrics, handlers and a helpful contrib section! \n",
    "\n",
    "Below we import the following:\n",
    "* **Engine**: Runs a given process_function over each batch of a dataset, emitting events as it goes.\n",
    "* **Events**: Allows users to attach functions to an `Engine` to fire functions at a specific event. Eg: `EPOCH_COMPLETED`, `ITERATION_STARTED`, etc.\n",
    "* **Accuracy**: Metric to calculate accuracy over a dataset, for binary, multiclass, multilabel cases. \n",
    "* **Loss**: General metric that takes a loss function as a parameter, calculate loss over a dataset.\n",
    "* **RunningAverage**: General metric to attach to Engine during training.\n",
    "* **global_step_from_engine**: Helper method to setup global_step_transform function using another engine.\n",
    "* **EarlyStopping**: Handler to stop training based on a score function.\n",
    "* **ProgressBar**: Utility to easily track the progress of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YodbjXPi6psK"
   },
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage, ConfusionMatrix\n",
    "from ignite.handlers import global_step_from_engine, EarlyStopping\n",
    "from ignite.contrib.handlers import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then finally we import `aim`'s adapter designed for ignite to be able to track training metrics and h-params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from aim.pytorch_ignite import AimLogger"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vO53-X98abx-"
   },
   "source": [
    "The code below first sets up transform using `torhvision transfroms` for converting images to pytorch tensors and normalizing the images.\n",
    "\n",
    "Next, we use `torchvision datasets` for dowloading the fashion mnist dataset and applying transforms which we defined above.\n",
    "\n",
    "* `trainset` contains the training data.\n",
    "* `validationset` contains the validation data\n",
    "\n",
    "Next, we use `pytorch dataloader` for making dataloader from the train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2dqyxnr6psU"
   },
   "outputs": [],
   "source": [
    "# transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('./data', download=True, train=True, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "validationset = datasets.FashionMNIST('./data', download=True, train=False, transform=transform)\n",
    "val_loader = DataLoader(validationset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cvzm-Oe1abyB"
   },
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uf4AVJ44abyC"
   },
   "source": [
    "Explanation of Model Architecture\n",
    "\n",
    "* [Convolutional layers](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), the Convolutional layer is used to create a convolution kernel that is convolved with the layer input to produce a tensor of outputs.\n",
    "* [Maxpooling layers](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html), the Maxpooling layer is used to downsample an input representation keeping the most active pixels from the previous layer.\n",
    "* The usual [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) + [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html) layers to avoid overfitting and produce a 10-dim output.\n",
    "* We had used [Relu](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) Non Linearity for the model and [logsoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html) at the last layer because we are going to use the [NLLL loss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AST_DtTC6psh"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.convlayer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3,padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.convlayer2 = nn.Sequential(\n",
    "            nn.Conv2d(32,64,3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(64*6*6,600)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(600, 120)\n",
    "        self.fc3 = nn.Linear(120, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convlayer1(x)\n",
    "        x = self.convlayer2(x)\n",
    "        x = x.view(-1,64*6*6)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqvY1QLBabyE"
   },
   "source": [
    "### Creating Model, Optimizer and Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8z2zUS5zabyF"
   },
   "source": [
    "Below we create an instance of the CNN model. The model is placed on a device and then a loss function of `negative log likelihood loss` and `Adam optimizer` with learning rate of 0.001 are setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXyVBBYw6pst"
   },
   "outputs": [],
   "source": [
    "# creating model,and defining optimizer and loss\n",
    "model = CNN()\n",
    "# moving model to gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o69S4bEvabyI"
   },
   "source": [
    "### Training and Evaluating using Ignite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PALFpjYoabyJ"
   },
   "source": [
    "### Instantiating Training and Evaluating Engines\n",
    "\n",
    "Below we create 3 engines, a trainer, an evaluator for the training set and an evaluator for the validation set, by using the `create_supervised_trainer` and `create_supervised_evaluator` and passing the required arguments.\n",
    "\n",
    "We import the metrics from `ignite.metrics` which we want to calculate for the model. Like `Accuracy`, `ConfusionMatrix`, and `Loss` and we pass them to `evaluator` engines which will calculate these metrics for each iteration.\n",
    "\n",
    "* `training_history`: it stores the training loss and accuracy\n",
    "* `validation_history`:it stores the validation loss and accuracy\n",
    "* `last_epoch`: it stores the last epoch untill the model is trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7EaSDMW6ps5"
   },
   "outputs": [],
   "source": [
    "# defining the number of epochs\n",
    "epochs = 12\n",
    "# creating trainer,evaluator\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "metrics = {\n",
    "    'accuracy':Accuracy(),\n",
    "    'nll':Loss(criterion),\n",
    "    'cm':ConfusionMatrix(num_classes=10)\n",
    "}\n",
    "train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "val_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "training_history = {'accuracy':[],'loss':[]}\n",
    "validation_history = {'accuracy':[],'loss':[]}\n",
    "last_epoch = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aonym06RabyL"
   },
   "source": [
    "### Metrics - RunningAverage\n",
    "\n",
    "To start, we will attach a metric of `RunningAverage` to track a running average of the scalar loss output for each batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LXaAfga6ptD"
   },
   "outputs": [],
   "source": [
    "RunningAverage(output_transform=lambda x: x).attach(trainer, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vDAID6BabyP"
   },
   "source": [
    "### EarlyStopping - Tracking Validation Loss\n",
    "\n",
    "Now we will setup a `EarlyStopping` handler for this training process. EarlyStopping requires a score_function that allows the user to define whatever criteria to stop trainig. In this case, if the loss of the validation set does not decrease in 10 epochs, the training process will stop early. Since the `EarlyStopping` handler relies on the validation loss, it's attached to the `val_evaluator`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mcIq0lc6ptN"
   },
   "outputs": [],
   "source": [
    "def score_function(engine):\n",
    "    val_loss = engine.state.metrics['nll']\n",
    "    return -val_loss\n",
    "\n",
    "handler = EarlyStopping(patience=10, score_function=score_function, trainer=trainer)\n",
    "val_evaluator.add_event_handler(Events.COMPLETED, handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The below function will trigger training evaluation and validation after each epoch is completed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    train_evaluator.run(train_loader)\n",
    "    val_evaluator.run(val_loader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attaching Custom Functions to Engine at specific Events\n",
    "\n",
    "Below you will see how to create and use `AimLogger`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a logger"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aim_logger = AimLogger()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Log experiment parameters:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aim_logger.log_params({\n",
    "    \"model\": model.__class__.__name__,\n",
    "    \"pytorch_version\": str(torch.__version__),\n",
    "    \"ignite_version\": str(ignite.__version__),\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Attach the logger to the trainer to log training loss at each iteration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aim_logger.attach_output_handler(\n",
    "    trainer,\n",
    "    event_name=Events.ITERATION_COMPLETED,\n",
    "    tag=\"train\",\n",
    "    output_transform=lambda loss: {'loss': loss}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Attach the logger to the evaluator on the training dataset and log `NLL`, `Accuracy` metrics after each epoch.\n",
    "We setup `global_step_transform=global_step_from_engine(trainer)` to take the epoch\n",
    "of the `trainer` instead of `train_evaluator`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aim_logger.attach_output_handler(\n",
    "    train_evaluator,\n",
    "    event_name=Events.EPOCH_COMPLETED,\n",
    "    tag=\"train\",\n",
    "    metric_names=[\"nll\", \"accuracy\"],\n",
    "    global_step_transform=global_step_from_engine(trainer),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Attach the logger to the evaluator on the validation dataset and log `NLL`, `Accuracy` metrics after\n",
    "each epoch.\n",
    "We setup `global_step_transform=global_step_from_engine(trainer)` to take the epoch of the\n",
    "`trainer` instead of `evaluator`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aim_logger.attach_output_handler(\n",
    "    val_evaluator,\n",
    "    event_name=Events.EPOCH_COMPLETED,\n",
    "    tag=\"val\",\n",
    "    metric_names=[\"nll\", \"accuracy\"],\n",
    "    global_step_transform=global_step_from_engine(trainer),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Attach the logger to the trainer to log optimizer's parameters, e.g. learning rate at each epoch iteration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aim_logger.attach_opt_params_handler(\n",
    "    trainer,\n",
    "    event_name=Events.EPOCH_STARTED,\n",
    "    optimizer=optimizer,\n",
    "    param_name='lr'  # optional\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YK6lGPHabyY"
   },
   "source": [
    "### Run Engine\n",
    "\n",
    "Finally, we'll attach a progress bar to the trainer via `loss` metric to track the training progress from console output\n",
    "and then we will run the trainer for 12 epochs and monitor results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pbar = ProgressBar(persist=True, bar_format=\"\")\n",
    "pbar.attach(trainer, ['loss'])\n",
    "trainer.run(train_loader, max_epochs=epochs)",
    "aim_logger.experiment.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploring the tracked data with Aim\n",
    "\n",
    "Load Aim extension for notebooks:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext aim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run `%aim up` to open Aim UI in the notebook:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%aim up"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWXOEpQ4abyv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### References\n",
    "* [Convolutional Neural Networks for Classifying Fashion-MNIST Dataset using Ignite](https://github.com/pytorch/ignite/blob/master/examples/notebooks/FashionMNIST.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FashionMNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
